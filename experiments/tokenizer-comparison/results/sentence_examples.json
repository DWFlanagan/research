{
  "1706.03762_character": [
    {
      "text": "Each layer has two sub-layers.",
      "tokens": [
        "E",
        "a",
        "c",
        "h",
        " ",
        "l",
        "a",
        "y",
        "e",
        "r",
        " ",
        "h",
        "a",
        "s",
        " ",
        "t",
        "w",
        "o",
        " ",
        "s",
        "u",
        "b",
        "-",
        "l",
        "a",
        "y",
        "e",
        "r",
        "s",
        "."
      ],
      "token_count": 30,
      "char_count": 30
    },
    {
      "text": "We call our particular attention \"Scaled Dot-Product Attention\".",
      "tokens": [
        "W",
        "e",
        " ",
        "c",
        "a",
        "l",
        "l",
        " ",
        "o",
        "u",
        "r",
        " ",
        "p",
        "a",
        "r",
        "t",
        "i",
        "c",
        "u",
        "l",
        "a",
        "r",
        " ",
        "a",
        "t",
        "t",
        "e",
        "n",
        "t",
        "i",
        "o",
        "n",
        " ",
        "\"",
        "S",
        "c",
        "a",
        "l",
        "e",
        "d",
        " ",
        "D",
        "o",
        "t",
        "-",
        "P",
        "r",
        "o",
        "d",
        "u",
        "c",
        "t",
        " ",
        "A",
        "t",
        "t",
        "e",
        "n",
        "t",
        "i",
        "o",
        "n",
        "\"",
        "."
      ],
      "token_count": 64,
      "char_count": 64
    },
    {
      "text": "The best performing models also connect the encoder\nand decoder through an attention mechanism.",
      "tokens": [
        "T",
        "h",
        "e",
        " ",
        "b",
        "e",
        "s",
        "t",
        " ",
        "p",
        "e",
        "r",
        "f",
        "o",
        "r",
        "m",
        "i",
        "n",
        "g",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "s",
        " ",
        "a",
        "l",
        "s",
        "o",
        " ",
        "c",
        "o",
        "n",
        "n",
        "e",
        "c",
        "t",
        " ",
        "t",
        "h",
        "e",
        " ",
        "e",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        "\n",
        "a",
        "n",
        "d",
        " ",
        "d",
        "e",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "t",
        "h",
        "r",
        "o",
        "u",
        "g",
        "h",
        " ",
        "a",
        "n",
        " ",
        "a",
        "t",
        "t",
        "e",
        "n",
        "t",
        "i",
        "o",
        "n",
        " ",
        "m",
        "e",
        "c",
        "h",
        "a",
        "n",
        "i",
        "s",
        "m",
        "."
      ],
      "token_count": 95,
      "char_count": 95
    },
    {
      "text": "3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers.",
      "tokens": [
        "3",
        ".",
        "1",
        " ",
        "E",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "a",
        "n",
        "d",
        " ",
        "D",
        "e",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "S",
        "t",
        "a",
        "c",
        "k",
        "s",
        "\n",
        "E",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        ":",
        " ",
        "T",
        "h",
        "e",
        " ",
        "e",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "i",
        "s",
        " ",
        "c",
        "o",
        "m",
        "p",
        "o",
        "s",
        "e",
        "d",
        " ",
        "o",
        "f",
        " ",
        "a",
        " ",
        "s",
        "t",
        "a",
        "c",
        "k",
        " ",
        "o",
        "f",
        " ",
        "N",
        " ",
        "=",
        " ",
        "6",
        " ",
        "i",
        "d",
        "e",
        "n",
        "t",
        "i",
        "c",
        "a",
        "l",
        " ",
        "l",
        "a",
        "y",
        "e",
        "r",
        "s",
        "."
      ],
      "token_count": 101,
      "char_count": 101
    },
    {
      "text": "In all but a few cases, however, such attention mechanisms\nare used in conjunction with a recurrent network.",
      "tokens": [
        "I",
        "n",
        " ",
        "a",
        "l",
        "l",
        " ",
        "b",
        "u",
        "t",
        " ",
        "a",
        " ",
        "f",
        "e",
        "w",
        " ",
        "c",
        "a",
        "s",
        "e",
        "s",
        ",",
        " ",
        "h",
        "o",
        "w",
        "e",
        "v",
        "e",
        "r",
        ",",
        " ",
        "s",
        "u",
        "c",
        "h",
        " ",
        "a",
        "t",
        "t",
        "e",
        "n",
        "t",
        "i",
        "o",
        "n",
        " ",
        "m",
        "e",
        "c",
        "h",
        "a",
        "n",
        "i",
        "s",
        "m",
        "s",
        "\n",
        "a",
        "r",
        "e",
        " ",
        "u",
        "s",
        "e",
        "d",
        " ",
        "i",
        "n",
        " ",
        "c",
        "o",
        "n",
        "j",
        "u",
        "n",
        "c",
        "t",
        "i",
        "o",
        "n",
        " ",
        "w",
        "i",
        "t",
        "h",
        " ",
        "a",
        " ",
        "r",
        "e",
        "c",
        "u",
        "r",
        "r",
        "e",
        "n",
        "t",
        " ",
        "n",
        "e",
        "t",
        "w",
        "o",
        "r",
        "k",
        "."
      ],
      "token_count": 108,
      "char_count": 108
    },
    {
      "text": "Numerous efforts have since continued\nto push the boundaries of recurrent language models and encoder-decoder architectures.",
      "tokens": [
        "N",
        "u",
        "m",
        "e",
        "r",
        "o",
        "u",
        "s",
        " ",
        "e",
        "f",
        "f",
        "o",
        "r",
        "t",
        "s",
        " ",
        "h",
        "a",
        "v",
        "e",
        " ",
        "s",
        "i",
        "n",
        "c",
        "e",
        " ",
        "c",
        "o",
        "n",
        "t",
        "i",
        "n",
        "u",
        "e",
        "d",
        "\n",
        "t",
        "o",
        " ",
        "p",
        "u",
        "s",
        "h",
        " ",
        "t",
        "h",
        "e",
        " ",
        "b",
        "o",
        "u",
        "n",
        "d",
        "a",
        "r",
        "i",
        "e",
        "s",
        " ",
        "o",
        "f",
        " ",
        "r",
        "e",
        "c",
        "u",
        "r",
        "r",
        "e",
        "n",
        "t",
        " ",
        "l",
        "a",
        "n",
        "g",
        "u",
        "a",
        "g",
        "e",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "s",
        " ",
        "a",
        "n",
        "d",
        " ",
        "e",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        "-",
        "d",
        "e",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "a",
        "r",
        "c",
        "h",
        "i",
        "t",
        "e",
        "c",
        "t",
        "u",
        "r",
        "e",
        "s",
        "."
      ],
      "token_count": 124,
      "char_count": 124
    },
    {
      "text": "Here, the\nencoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous\nrepresentations z = (z1, ..., zn).",
      "tokens": [
        "H",
        "e",
        "r",
        "e",
        ",",
        " ",
        "t",
        "h",
        "e",
        "\n",
        "e",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "m",
        "a",
        "p",
        "s",
        " ",
        "a",
        "n",
        " ",
        "i",
        "n",
        "p",
        "u",
        "t",
        " ",
        "s",
        "e",
        "q",
        "u",
        "e",
        "n",
        "c",
        "e",
        " ",
        "o",
        "f",
        " ",
        "s",
        "y",
        "m",
        "b",
        "o",
        "l",
        " ",
        "r",
        "e",
        "p",
        "r",
        "e",
        "s",
        "e",
        "n",
        "t",
        "a",
        "t",
        "i",
        "o",
        "n",
        "s",
        " ",
        "(",
        "x",
        "1",
        ",",
        " ",
        ".",
        ".",
        ".",
        ",",
        " ",
        "x",
        "n",
        ")",
        " ",
        "t",
        "o",
        " ",
        "a",
        " ",
        "s",
        "e",
        "q",
        "u",
        "e",
        "n",
        "c",
        "e",
        " ",
        "o",
        "f",
        " ",
        "c",
        "o",
        "n",
        "t",
        "i",
        "n",
        "u",
        "o",
        "u",
        "s",
        "\n",
        "r",
        "e",
        "p",
        "r",
        "e",
        "s",
        "e",
        "n",
        "t",
        "a",
        "t",
        "i",
        "o",
        "n",
        "s",
        " ",
        "z",
        " ",
        "=",
        " ",
        "(",
        "z",
        "1",
        ",",
        " ",
        ".",
        ".",
        ".",
        ",",
        " ",
        "z",
        "n",
        ")",
        "."
      ],
      "token_count": 143,
      "char_count": 143
    },
    {
      "text": "The Transformer follows this overall architecture using stacked\nself-attention and point-wise, fully connected layers for both the encoder and decoder.",
      "tokens": [
        "T",
        "h",
        "e",
        " ",
        "T",
        "r",
        "a",
        "n",
        "s",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        " ",
        "f",
        "o",
        "l",
        "l",
        "o",
        "w",
        "s",
        " ",
        "t",
        "h",
        "i",
        "s",
        " ",
        "o",
        "v",
        "e",
        "r",
        "a",
        "l",
        "l",
        " ",
        "a",
        "r",
        "c",
        "h",
        "i",
        "t",
        "e",
        "c",
        "t",
        "u",
        "r",
        "e",
        " ",
        "u",
        "s",
        "i",
        "n",
        "g",
        " ",
        "s",
        "t",
        "a",
        "c",
        "k",
        "e",
        "d",
        "\n",
        "s",
        "e",
        "l",
        "f",
        "-",
        "a",
        "t",
        "t",
        "e",
        "n",
        "t",
        "i",
        "o",
        "n",
        " ",
        "a",
        "n",
        "d",
        " ",
        "p",
        "o",
        "i",
        "n",
        "t",
        "-",
        "w",
        "i",
        "s",
        "e",
        ",",
        " ",
        "f",
        "u",
        "l",
        "l",
        "y",
        " ",
        "c",
        "o",
        "n",
        "n",
        "e",
        "c",
        "t",
        "e",
        "d",
        " ",
        "l",
        "a",
        "y",
        "e",
        "r",
        "s",
        " ",
        "f",
        "o",
        "r",
        " ",
        "b",
        "o",
        "t",
        "h",
        " ",
        "t",
        "h",
        "e",
        " ",
        "e",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "a",
        "n",
        "d",
        " ",
        "d",
        "e",
        "c",
        "o",
        "d",
        "e",
        "r",
        "."
      ],
      "token_count": 151,
      "char_count": 151
    },
    {
      "text": "Our model achieves 28.4\nBLEU on the WMT 2014 English-to-German translation task, improving over the existing best results,\nincluding ensembles, by over 2 BLEU.",
      "tokens": [
        "O",
        "u",
        "r",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        " ",
        "a",
        "c",
        "h",
        "i",
        "e",
        "v",
        "e",
        "s",
        " ",
        "2",
        "8",
        ".",
        "4",
        "\n",
        "B",
        "L",
        "E",
        "U",
        " ",
        "o",
        "n",
        " ",
        "t",
        "h",
        "e",
        " ",
        "W",
        "M",
        "T",
        " ",
        "2",
        "0",
        "1",
        "4",
        " ",
        "E",
        "n",
        "g",
        "l",
        "i",
        "s",
        "h",
        "-",
        "t",
        "o",
        "-",
        "G",
        "e",
        "r",
        "m",
        "a",
        "n",
        " ",
        "t",
        "r",
        "a",
        "n",
        "s",
        "l",
        "a",
        "t",
        "i",
        "o",
        "n",
        " ",
        "t",
        "a",
        "s",
        "k",
        ",",
        " ",
        "i",
        "m",
        "p",
        "r",
        "o",
        "v",
        "i",
        "n",
        "g",
        " ",
        "o",
        "v",
        "e",
        "r",
        " ",
        "t",
        "h",
        "e",
        " ",
        "e",
        "x",
        "i",
        "s",
        "t",
        "i",
        "n",
        "g",
        " ",
        "b",
        "e",
        "s",
        "t",
        " ",
        "r",
        "e",
        "s",
        "u",
        "l",
        "t",
        "s",
        ",",
        "\n",
        "i",
        "n",
        "c",
        "l",
        "u",
        "d",
        "i",
        "n",
        "g",
        " ",
        "e",
        "n",
        "s",
        "e",
        "m",
        "b",
        "l",
        "e",
        "s",
        ",",
        " ",
        "b",
        "y",
        " ",
        "o",
        "v",
        "e",
        "r",
        " ",
        "2",
        " ",
        "B",
        "L",
        "E",
        "U",
        "."
      ],
      "token_count": 159,
      "char_count": 159
    },
    {
      "text": "In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.",
      "tokens": [
        "I",
        "n",
        " ",
        "a",
        "d",
        "d",
        "i",
        "t",
        "i",
        "o",
        "n",
        " ",
        "t",
        "o",
        " ",
        "t",
        "h",
        "e",
        " ",
        "t",
        "w",
        "o",
        "\n",
        "s",
        "u",
        "b",
        "-",
        "l",
        "a",
        "y",
        "e",
        "r",
        "s",
        " ",
        "i",
        "n",
        " ",
        "e",
        "a",
        "c",
        "h",
        " ",
        "e",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "l",
        "a",
        "y",
        "e",
        "r",
        ",",
        " ",
        "t",
        "h",
        "e",
        " ",
        "d",
        "e",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "i",
        "n",
        "s",
        "e",
        "r",
        "t",
        "s",
        " ",
        "a",
        " ",
        "t",
        "h",
        "i",
        "r",
        "d",
        " ",
        "s",
        "u",
        "b",
        "-",
        "l",
        "a",
        "y",
        "e",
        "r",
        ",",
        " ",
        "w",
        "h",
        "i",
        "c",
        "h",
        " ",
        "p",
        "e",
        "r",
        "f",
        "o",
        "r",
        "m",
        "s",
        " ",
        "m",
        "u",
        "l",
        "t",
        "i",
        "-",
        "h",
        "e",
        "a",
        "d",
        "\n",
        "a",
        "t",
        "t",
        "e",
        "n",
        "t",
        "i",
        "o",
        "n",
        " ",
        "o",
        "v",
        "e",
        "r",
        " ",
        "t",
        "h",
        "e",
        " ",
        "o",
        "u",
        "t",
        "p",
        "u",
        "t",
        " ",
        "o",
        "f",
        " ",
        "t",
        "h",
        "e",
        " ",
        "e",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "s",
        "t",
        "a",
        "c",
        "k",
        "."
      ],
      "token_count": 169,
      "char_count": 169
    }
  ],
  "1706.03762_whitespace": [
    {
      "text": "Each layer has two sub-layers.",
      "tokens": [
        "Each",
        "layer",
        "has",
        "two",
        "sub-layers."
      ],
      "token_count": 5,
      "char_count": 30
    },
    {
      "text": "We call our particular attention \"Scaled Dot-Product Attention\".",
      "tokens": [
        "We",
        "call",
        "our",
        "particular",
        "attention",
        "\"Scaled",
        "Dot-Product",
        "Attention\"."
      ],
      "token_count": 8,
      "char_count": 64
    },
    {
      "text": "The best performing models also connect the encoder\nand decoder through an attention mechanism.",
      "tokens": [
        "The",
        "best",
        "performing",
        "models",
        "also",
        "connect",
        "the",
        "encoder",
        "and",
        "decoder",
        "through",
        "an",
        "attention",
        "mechanism."
      ],
      "token_count": 14,
      "char_count": 95
    },
    {
      "text": "3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers.",
      "tokens": [
        "3.1",
        "Encoder",
        "and",
        "Decoder",
        "Stacks",
        "Encoder:",
        "The",
        "encoder",
        "is",
        "composed",
        "of",
        "a",
        "stack",
        "of",
        "N",
        "=",
        "6",
        "identical",
        "layers."
      ],
      "token_count": 19,
      "char_count": 101
    },
    {
      "text": "In all but a few cases, however, such attention mechanisms\nare used in conjunction with a recurrent network.",
      "tokens": [
        "In",
        "all",
        "but",
        "a",
        "few",
        "cases,",
        "however,",
        "such",
        "attention",
        "mechanisms",
        "are",
        "used",
        "in",
        "conjunction",
        "with",
        "a",
        "recurrent",
        "network."
      ],
      "token_count": 18,
      "char_count": 108
    },
    {
      "text": "Numerous efforts have since continued\nto push the boundaries of recurrent language models and encoder-decoder architectures.",
      "tokens": [
        "Numerous",
        "efforts",
        "have",
        "since",
        "continued",
        "to",
        "push",
        "the",
        "boundaries",
        "of",
        "recurrent",
        "language",
        "models",
        "and",
        "encoder-decoder",
        "architectures."
      ],
      "token_count": 16,
      "char_count": 124
    },
    {
      "text": "Here, the\nencoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous\nrepresentations z = (z1, ..., zn).",
      "tokens": [
        "Here,",
        "the",
        "encoder",
        "maps",
        "an",
        "input",
        "sequence",
        "of",
        "symbol",
        "representations",
        "(x1,",
        "...,",
        "xn)",
        "to",
        "a",
        "sequence",
        "of",
        "continuous",
        "representations",
        "z",
        "=",
        "(z1,",
        "...,",
        "zn)."
      ],
      "token_count": 24,
      "char_count": 143
    },
    {
      "text": "The Transformer follows this overall architecture using stacked\nself-attention and point-wise, fully connected layers for both the encoder and decoder.",
      "tokens": [
        "The",
        "Transformer",
        "follows",
        "this",
        "overall",
        "architecture",
        "using",
        "stacked",
        "self-attention",
        "and",
        "point-wise,",
        "fully",
        "connected",
        "layers",
        "for",
        "both",
        "the",
        "encoder",
        "and",
        "decoder."
      ],
      "token_count": 20,
      "char_count": 151
    },
    {
      "text": "Our model achieves 28.4\nBLEU on the WMT 2014 English-to-German translation task, improving over the existing best results,\nincluding ensembles, by over 2 BLEU.",
      "tokens": [
        "Our",
        "model",
        "achieves",
        "28.4",
        "BLEU",
        "on",
        "the",
        "WMT",
        "2014",
        "English-to-German",
        "translation",
        "task,",
        "improving",
        "over",
        "the",
        "existing",
        "best",
        "results,",
        "including",
        "ensembles,",
        "by",
        "over",
        "2",
        "BLEU."
      ],
      "token_count": 24,
      "char_count": 159
    },
    {
      "text": "In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.",
      "tokens": [
        "In",
        "addition",
        "to",
        "the",
        "two",
        "sub-layers",
        "in",
        "each",
        "encoder",
        "layer,",
        "the",
        "decoder",
        "inserts",
        "a",
        "third",
        "sub-layer,",
        "which",
        "performs",
        "multi-head",
        "attention",
        "over",
        "the",
        "output",
        "of",
        "the",
        "encoder",
        "stack."
      ],
      "token_count": 27,
      "char_count": 169
    }
  ],
  "1810.04805_character": [
    {
      "text": "We refer to this procedure as a \"masked LM\" (MLM).",
      "tokens": [
        "W",
        "e",
        " ",
        "r",
        "e",
        "f",
        "e",
        "r",
        " ",
        "t",
        "o",
        " ",
        "t",
        "h",
        "i",
        "s",
        " ",
        "p",
        "r",
        "o",
        "c",
        "e",
        "d",
        "u",
        "r",
        "e",
        " ",
        "a",
        "s",
        " ",
        "a",
        " ",
        "\"",
        "m",
        "a",
        "s",
        "k",
        "e",
        "d",
        " ",
        "L",
        "M",
        "\"",
        " ",
        "(",
        "M",
        "L",
        "M",
        ")",
        "."
      ],
      "token_count": 50,
      "char_count": 50
    },
    {
      "text": "3 BERT\n\nWe introduce BERT and its detailed implementation in this section.",
      "tokens": [
        "3",
        " ",
        "B",
        "E",
        "R",
        "T",
        "\n",
        "\n",
        "W",
        "e",
        " ",
        "i",
        "n",
        "t",
        "r",
        "o",
        "d",
        "u",
        "c",
        "e",
        " ",
        "B",
        "E",
        "R",
        "T",
        " ",
        "a",
        "n",
        "d",
        " ",
        "i",
        "t",
        "s",
        " ",
        "d",
        "e",
        "t",
        "a",
        "i",
        "l",
        "e",
        "d",
        " ",
        "i",
        "m",
        "p",
        "l",
        "e",
        "m",
        "e",
        "n",
        "t",
        "a",
        "t",
        "i",
        "o",
        "n",
        " ",
        "i",
        "n",
        " ",
        "t",
        "h",
        "i",
        "s",
        " ",
        "s",
        "e",
        "c",
        "t",
        "i",
        "o",
        "n",
        "."
      ],
      "token_count": 74,
      "char_count": 74
    },
    {
      "text": "In all of our\nexperiments, we mask 15% of all WordPiece tokens in each sequence at random.",
      "tokens": [
        "I",
        "n",
        " ",
        "a",
        "l",
        "l",
        " ",
        "o",
        "f",
        " ",
        "o",
        "u",
        "r",
        "\n",
        "e",
        "x",
        "p",
        "e",
        "r",
        "i",
        "m",
        "e",
        "n",
        "t",
        "s",
        ",",
        " ",
        "w",
        "e",
        " ",
        "m",
        "a",
        "s",
        "k",
        " ",
        "1",
        "5",
        "%",
        " ",
        "o",
        "f",
        " ",
        "a",
        "l",
        "l",
        " ",
        "W",
        "o",
        "r",
        "d",
        "P",
        "i",
        "e",
        "c",
        "e",
        " ",
        "t",
        "o",
        "k",
        "e",
        "n",
        "s",
        " ",
        "i",
        "n",
        " ",
        "e",
        "a",
        "c",
        "h",
        " ",
        "s",
        "e",
        "q",
        "u",
        "e",
        "n",
        "c",
        "e",
        " ",
        "a",
        "t",
        " ",
        "r",
        "a",
        "n",
        "d",
        "o",
        "m",
        "."
      ],
      "token_count": 90,
      "char_count": 90
    },
    {
      "text": "Sentence pairs are\npacked together into a single sequence separated with a special token ([SEP]).",
      "tokens": [
        "S",
        "e",
        "n",
        "t",
        "e",
        "n",
        "c",
        "e",
        " ",
        "p",
        "a",
        "i",
        "r",
        "s",
        " ",
        "a",
        "r",
        "e",
        "\n",
        "p",
        "a",
        "c",
        "k",
        "e",
        "d",
        " ",
        "t",
        "o",
        "g",
        "e",
        "t",
        "h",
        "e",
        "r",
        " ",
        "i",
        "n",
        "t",
        "o",
        " ",
        "a",
        " ",
        "s",
        "i",
        "n",
        "g",
        "l",
        "e",
        " ",
        "s",
        "e",
        "q",
        "u",
        "e",
        "n",
        "c",
        "e",
        " ",
        "s",
        "e",
        "p",
        "a",
        "r",
        "a",
        "t",
        "e",
        "d",
        " ",
        "w",
        "i",
        "t",
        "h",
        " ",
        "a",
        " ",
        "s",
        "p",
        "e",
        "c",
        "i",
        "a",
        "l",
        " ",
        "t",
        "o",
        "k",
        "e",
        "n",
        " ",
        "(",
        "[",
        "S",
        "E",
        "P",
        "]",
        ")",
        "."
      ],
      "token_count": 97,
      "char_count": 97
    },
    {
      "text": "3.1 Pre-training BERT\nWe pre-train BERT using two unsupervised tasks: Masked LM and Next Sentence Prediction.",
      "tokens": [
        "3",
        ".",
        "1",
        " ",
        "P",
        "r",
        "e",
        "-",
        "t",
        "r",
        "a",
        "i",
        "n",
        "i",
        "n",
        "g",
        " ",
        "B",
        "E",
        "R",
        "T",
        "\n",
        "W",
        "e",
        " ",
        "p",
        "r",
        "e",
        "-",
        "t",
        "r",
        "a",
        "i",
        "n",
        " ",
        "B",
        "E",
        "R",
        "T",
        " ",
        "u",
        "s",
        "i",
        "n",
        "g",
        " ",
        "t",
        "w",
        "o",
        " ",
        "u",
        "n",
        "s",
        "u",
        "p",
        "e",
        "r",
        "v",
        "i",
        "s",
        "e",
        "d",
        " ",
        "t",
        "a",
        "s",
        "k",
        "s",
        ":",
        " ",
        "M",
        "a",
        "s",
        "k",
        "e",
        "d",
        " ",
        "L",
        "M",
        " ",
        "a",
        "n",
        "d",
        " ",
        "N",
        "e",
        "x",
        "t",
        " ",
        "S",
        "e",
        "n",
        "t",
        "e",
        "n",
        "c",
        "e",
        " ",
        "P",
        "r",
        "e",
        "d",
        "i",
        "c",
        "t",
        "i",
        "o",
        "n",
        "."
      ],
      "token_count": 109,
      "char_count": 109
    },
    {
      "text": "1 Introduction\nLanguage model pre-training has been shown to be effective for improving many natural language\nprocessing tasks.",
      "tokens": [
        "1",
        " ",
        "I",
        "n",
        "t",
        "r",
        "o",
        "d",
        "u",
        "c",
        "t",
        "i",
        "o",
        "n",
        "\n",
        "L",
        "a",
        "n",
        "g",
        "u",
        "a",
        "g",
        "e",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        " ",
        "p",
        "r",
        "e",
        "-",
        "t",
        "r",
        "a",
        "i",
        "n",
        "i",
        "n",
        "g",
        " ",
        "h",
        "a",
        "s",
        " ",
        "b",
        "e",
        "e",
        "n",
        " ",
        "s",
        "h",
        "o",
        "w",
        "n",
        " ",
        "t",
        "o",
        " ",
        "b",
        "e",
        " ",
        "e",
        "f",
        "f",
        "e",
        "c",
        "t",
        "i",
        "v",
        "e",
        " ",
        "f",
        "o",
        "r",
        " ",
        "i",
        "m",
        "p",
        "r",
        "o",
        "v",
        "i",
        "n",
        "g",
        " ",
        "m",
        "a",
        "n",
        "y",
        " ",
        "n",
        "a",
        "t",
        "u",
        "r",
        "a",
        "l",
        " ",
        "l",
        "a",
        "n",
        "g",
        "u",
        "a",
        "g",
        "e",
        "\n",
        "p",
        "r",
        "o",
        "c",
        "e",
        "s",
        "s",
        "i",
        "n",
        "g",
        " ",
        "t",
        "a",
        "s",
        "k",
        "s",
        "."
      ],
      "token_count": 127,
      "char_count": 127
    },
    {
      "text": "BERT alleviates the previously mentioned unidirectionality\nconstraint by using a \"masked language model\" (MLM) pre-training objective.",
      "tokens": [
        "B",
        "E",
        "R",
        "T",
        " ",
        "a",
        "l",
        "l",
        "e",
        "v",
        "i",
        "a",
        "t",
        "e",
        "s",
        " ",
        "t",
        "h",
        "e",
        " ",
        "p",
        "r",
        "e",
        "v",
        "i",
        "o",
        "u",
        "s",
        "l",
        "y",
        " ",
        "m",
        "e",
        "n",
        "t",
        "i",
        "o",
        "n",
        "e",
        "d",
        " ",
        "u",
        "n",
        "i",
        "d",
        "i",
        "r",
        "e",
        "c",
        "t",
        "i",
        "o",
        "n",
        "a",
        "l",
        "i",
        "t",
        "y",
        "\n",
        "c",
        "o",
        "n",
        "s",
        "t",
        "r",
        "a",
        "i",
        "n",
        "t",
        " ",
        "b",
        "y",
        " ",
        "u",
        "s",
        "i",
        "n",
        "g",
        " ",
        "a",
        " ",
        "\"",
        "m",
        "a",
        "s",
        "k",
        "e",
        "d",
        " ",
        "l",
        "a",
        "n",
        "g",
        "u",
        "a",
        "g",
        "e",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "\"",
        " ",
        "(",
        "M",
        "L",
        "M",
        ")",
        " ",
        "p",
        "r",
        "e",
        "-",
        "t",
        "r",
        "a",
        "i",
        "n",
        "i",
        "n",
        "g",
        " ",
        "o",
        "b",
        "j",
        "e",
        "c",
        "t",
        "i",
        "v",
        "e",
        "."
      ],
      "token_count": 134,
      "char_count": 134
    },
    {
      "text": "The major limitation is that standard language models are unidirectional,\nand this limits the choice of architectures that can be used during pre-training.",
      "tokens": [
        "T",
        "h",
        "e",
        " ",
        "m",
        "a",
        "j",
        "o",
        "r",
        " ",
        "l",
        "i",
        "m",
        "i",
        "t",
        "a",
        "t",
        "i",
        "o",
        "n",
        " ",
        "i",
        "s",
        " ",
        "t",
        "h",
        "a",
        "t",
        " ",
        "s",
        "t",
        "a",
        "n",
        "d",
        "a",
        "r",
        "d",
        " ",
        "l",
        "a",
        "n",
        "g",
        "u",
        "a",
        "g",
        "e",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "s",
        " ",
        "a",
        "r",
        "e",
        " ",
        "u",
        "n",
        "i",
        "d",
        "i",
        "r",
        "e",
        "c",
        "t",
        "i",
        "o",
        "n",
        "a",
        "l",
        ",",
        "\n",
        "a",
        "n",
        "d",
        " ",
        "t",
        "h",
        "i",
        "s",
        " ",
        "l",
        "i",
        "m",
        "i",
        "t",
        "s",
        " ",
        "t",
        "h",
        "e",
        " ",
        "c",
        "h",
        "o",
        "i",
        "c",
        "e",
        " ",
        "o",
        "f",
        " ",
        "a",
        "r",
        "c",
        "h",
        "i",
        "t",
        "e",
        "c",
        "t",
        "u",
        "r",
        "e",
        "s",
        " ",
        "t",
        "h",
        "a",
        "t",
        " ",
        "c",
        "a",
        "n",
        " ",
        "b",
        "e",
        " ",
        "u",
        "s",
        "e",
        "d",
        " ",
        "d",
        "u",
        "r",
        "i",
        "n",
        "g",
        " ",
        "p",
        "r",
        "e",
        "-",
        "t",
        "r",
        "a",
        "i",
        "n",
        "i",
        "n",
        "g",
        "."
      ],
      "token_count": 155,
      "char_count": 155
    },
    {
      "text": "These include sentence-level tasks such as natural language inference and\nparaphrasing, as well as token-level tasks such as named entity recognition and question answering.",
      "tokens": [
        "T",
        "h",
        "e",
        "s",
        "e",
        " ",
        "i",
        "n",
        "c",
        "l",
        "u",
        "d",
        "e",
        " ",
        "s",
        "e",
        "n",
        "t",
        "e",
        "n",
        "c",
        "e",
        "-",
        "l",
        "e",
        "v",
        "e",
        "l",
        " ",
        "t",
        "a",
        "s",
        "k",
        "s",
        " ",
        "s",
        "u",
        "c",
        "h",
        " ",
        "a",
        "s",
        " ",
        "n",
        "a",
        "t",
        "u",
        "r",
        "a",
        "l",
        " ",
        "l",
        "a",
        "n",
        "g",
        "u",
        "a",
        "g",
        "e",
        " ",
        "i",
        "n",
        "f",
        "e",
        "r",
        "e",
        "n",
        "c",
        "e",
        " ",
        "a",
        "n",
        "d",
        "\n",
        "p",
        "a",
        "r",
        "a",
        "p",
        "h",
        "r",
        "a",
        "s",
        "i",
        "n",
        "g",
        ",",
        " ",
        "a",
        "s",
        " ",
        "w",
        "e",
        "l",
        "l",
        " ",
        "a",
        "s",
        " ",
        "t",
        "o",
        "k",
        "e",
        "n",
        "-",
        "l",
        "e",
        "v",
        "e",
        "l",
        " ",
        "t",
        "a",
        "s",
        "k",
        "s",
        " ",
        "s",
        "u",
        "c",
        "h",
        " ",
        "a",
        "s",
        " ",
        "n",
        "a",
        "m",
        "e",
        "d",
        " ",
        "e",
        "n",
        "t",
        "i",
        "t",
        "y",
        " ",
        "r",
        "e",
        "c",
        "o",
        "g",
        "n",
        "i",
        "t",
        "i",
        "o",
        "n",
        " ",
        "a",
        "n",
        "d",
        " ",
        "q",
        "u",
        "e",
        "s",
        "t",
        "i",
        "o",
        "n",
        " ",
        "a",
        "n",
        "s",
        "w",
        "e",
        "r",
        "i",
        "n",
        "g",
        "."
      ],
      "token_count": 173,
      "char_count": 173
    },
    {
      "text": "6 Conclusion\nRecent empirical improvements due to transfer learning with language models have demonstrated that\nrich, unsupervised pre-training is an integral part of many language understanding systems.",
      "tokens": [
        "6",
        " ",
        "C",
        "o",
        "n",
        "c",
        "l",
        "u",
        "s",
        "i",
        "o",
        "n",
        "\n",
        "R",
        "e",
        "c",
        "e",
        "n",
        "t",
        " ",
        "e",
        "m",
        "p",
        "i",
        "r",
        "i",
        "c",
        "a",
        "l",
        " ",
        "i",
        "m",
        "p",
        "r",
        "o",
        "v",
        "e",
        "m",
        "e",
        "n",
        "t",
        "s",
        " ",
        "d",
        "u",
        "e",
        " ",
        "t",
        "o",
        " ",
        "t",
        "r",
        "a",
        "n",
        "s",
        "f",
        "e",
        "r",
        " ",
        "l",
        "e",
        "a",
        "r",
        "n",
        "i",
        "n",
        "g",
        " ",
        "w",
        "i",
        "t",
        "h",
        " ",
        "l",
        "a",
        "n",
        "g",
        "u",
        "a",
        "g",
        "e",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "s",
        " ",
        "h",
        "a",
        "v",
        "e",
        " ",
        "d",
        "e",
        "m",
        "o",
        "n",
        "s",
        "t",
        "r",
        "a",
        "t",
        "e",
        "d",
        " ",
        "t",
        "h",
        "a",
        "t",
        "\n",
        "r",
        "i",
        "c",
        "h",
        ",",
        " ",
        "u",
        "n",
        "s",
        "u",
        "p",
        "e",
        "r",
        "v",
        "i",
        "s",
        "e",
        "d",
        " ",
        "p",
        "r",
        "e",
        "-",
        "t",
        "r",
        "a",
        "i",
        "n",
        "i",
        "n",
        "g",
        " ",
        "i",
        "s",
        " ",
        "a",
        "n",
        " ",
        "i",
        "n",
        "t",
        "e",
        "g",
        "r",
        "a",
        "l",
        " ",
        "p",
        "a",
        "r",
        "t",
        " ",
        "o",
        "f",
        " ",
        "m",
        "a",
        "n",
        "y",
        " ",
        "l",
        "a",
        "n",
        "g",
        "u",
        "a",
        "g",
        "e",
        " ",
        "u",
        "n",
        "d",
        "e",
        "r",
        "s",
        "t",
        "a",
        "n",
        "d",
        "i",
        "n",
        "g",
        " ",
        "s",
        "y",
        "s",
        "t",
        "e",
        "m",
        "s",
        "."
      ],
      "token_count": 203,
      "char_count": 203
    }
  ],
  "1810.04805_whitespace": [
    {
      "text": "We refer to this procedure as a \"masked LM\" (MLM).",
      "tokens": [
        "We",
        "refer",
        "to",
        "this",
        "procedure",
        "as",
        "a",
        "\"masked",
        "LM\"",
        "(MLM)."
      ],
      "token_count": 10,
      "char_count": 50
    },
    {
      "text": "3 BERT\n\nWe introduce BERT and its detailed implementation in this section.",
      "tokens": [
        "3",
        "BERT",
        "We",
        "introduce",
        "BERT",
        "and",
        "its",
        "detailed",
        "implementation",
        "in",
        "this",
        "section."
      ],
      "token_count": 12,
      "char_count": 74
    },
    {
      "text": "In all of our\nexperiments, we mask 15% of all WordPiece tokens in each sequence at random.",
      "tokens": [
        "In",
        "all",
        "of",
        "our",
        "experiments,",
        "we",
        "mask",
        "15%",
        "of",
        "all",
        "WordPiece",
        "tokens",
        "in",
        "each",
        "sequence",
        "at",
        "random."
      ],
      "token_count": 17,
      "char_count": 90
    },
    {
      "text": "Sentence pairs are\npacked together into a single sequence separated with a special token ([SEP]).",
      "tokens": [
        "Sentence",
        "pairs",
        "are",
        "packed",
        "together",
        "into",
        "a",
        "single",
        "sequence",
        "separated",
        "with",
        "a",
        "special",
        "token",
        "([SEP])."
      ],
      "token_count": 15,
      "char_count": 97
    },
    {
      "text": "3.1 Pre-training BERT\nWe pre-train BERT using two unsupervised tasks: Masked LM and Next Sentence Prediction.",
      "tokens": [
        "3.1",
        "Pre-training",
        "BERT",
        "We",
        "pre-train",
        "BERT",
        "using",
        "two",
        "unsupervised",
        "tasks:",
        "Masked",
        "LM",
        "and",
        "Next",
        "Sentence",
        "Prediction."
      ],
      "token_count": 16,
      "char_count": 109
    },
    {
      "text": "1 Introduction\nLanguage model pre-training has been shown to be effective for improving many natural language\nprocessing tasks.",
      "tokens": [
        "1",
        "Introduction",
        "Language",
        "model",
        "pre-training",
        "has",
        "been",
        "shown",
        "to",
        "be",
        "effective",
        "for",
        "improving",
        "many",
        "natural",
        "language",
        "processing",
        "tasks."
      ],
      "token_count": 18,
      "char_count": 127
    },
    {
      "text": "BERT alleviates the previously mentioned unidirectionality\nconstraint by using a \"masked language model\" (MLM) pre-training objective.",
      "tokens": [
        "BERT",
        "alleviates",
        "the",
        "previously",
        "mentioned",
        "unidirectionality",
        "constraint",
        "by",
        "using",
        "a",
        "\"masked",
        "language",
        "model\"",
        "(MLM)",
        "pre-training",
        "objective."
      ],
      "token_count": 16,
      "char_count": 134
    },
    {
      "text": "The major limitation is that standard language models are unidirectional,\nand this limits the choice of architectures that can be used during pre-training.",
      "tokens": [
        "The",
        "major",
        "limitation",
        "is",
        "that",
        "standard",
        "language",
        "models",
        "are",
        "unidirectional,",
        "and",
        "this",
        "limits",
        "the",
        "choice",
        "of",
        "architectures",
        "that",
        "can",
        "be",
        "used",
        "during",
        "pre-training."
      ],
      "token_count": 23,
      "char_count": 155
    },
    {
      "text": "These include sentence-level tasks such as natural language inference and\nparaphrasing, as well as token-level tasks such as named entity recognition and question answering.",
      "tokens": [
        "These",
        "include",
        "sentence-level",
        "tasks",
        "such",
        "as",
        "natural",
        "language",
        "inference",
        "and",
        "paraphrasing,",
        "as",
        "well",
        "as",
        "token-level",
        "tasks",
        "such",
        "as",
        "named",
        "entity",
        "recognition",
        "and",
        "question",
        "answering."
      ],
      "token_count": 24,
      "char_count": 173
    },
    {
      "text": "6 Conclusion\nRecent empirical improvements due to transfer learning with language models have demonstrated that\nrich, unsupervised pre-training is an integral part of many language understanding systems.",
      "tokens": [
        "6",
        "Conclusion",
        "Recent",
        "empirical",
        "improvements",
        "due",
        "to",
        "transfer",
        "learning",
        "with",
        "language",
        "models",
        "have",
        "demonstrated",
        "that",
        "rich,",
        "unsupervised",
        "pre-training",
        "is",
        "an",
        "integral",
        "part",
        "of",
        "many",
        "language",
        "understanding",
        "systems."
      ],
      "token_count": 27,
      "char_count": 203
    }
  ],
  "2105.13626_character": [
    {
      "text": "We call the resulting models ByT5.",
      "tokens": [
        "W",
        "e",
        " ",
        "c",
        "a",
        "l",
        "l",
        " ",
        "t",
        "h",
        "e",
        " ",
        "r",
        "e",
        "s",
        "u",
        "l",
        "t",
        "i",
        "n",
        "g",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "s",
        " ",
        "B",
        "y",
        "T",
        "5",
        "."
      ],
      "token_count": 34,
      "char_count": 34
    },
    {
      "text": "3 Approach\nWe base our models on T5, an encoder-decoder Transformer.",
      "tokens": [
        "3",
        " ",
        "A",
        "p",
        "p",
        "r",
        "o",
        "a",
        "c",
        "h",
        "\n",
        "W",
        "e",
        " ",
        "b",
        "a",
        "s",
        "e",
        " ",
        "o",
        "u",
        "r",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "s",
        " ",
        "o",
        "n",
        " ",
        "T",
        "5",
        ",",
        " ",
        "a",
        "n",
        " ",
        "e",
        "n",
        "c",
        "o",
        "d",
        "e",
        "r",
        "-",
        "d",
        "e",
        "c",
        "o",
        "d",
        "e",
        "r",
        " ",
        "T",
        "r",
        "a",
        "n",
        "s",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        "."
      ],
      "token_count": 68,
      "char_count": 68
    },
    {
      "text": "We demonstrated that byte-level models\n\nare significantly more robust to noise.",
      "tokens": [
        "W",
        "e",
        " ",
        "d",
        "e",
        "m",
        "o",
        "n",
        "s",
        "t",
        "r",
        "a",
        "t",
        "e",
        "d",
        " ",
        "t",
        "h",
        "a",
        "t",
        " ",
        "b",
        "y",
        "t",
        "e",
        "-",
        "l",
        "e",
        "v",
        "e",
        "l",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "s",
        "\n",
        "\n",
        "a",
        "r",
        "e",
        " ",
        "s",
        "i",
        "g",
        "n",
        "i",
        "f",
        "i",
        "c",
        "a",
        "n",
        "t",
        "l",
        "y",
        " ",
        "m",
        "o",
        "r",
        "e",
        " ",
        "r",
        "o",
        "b",
        "u",
        "s",
        "t",
        " ",
        "t",
        "o",
        " ",
        "n",
        "o",
        "i",
        "s",
        "e",
        "."
      ],
      "token_count": 79,
      "char_count": 79
    },
    {
      "text": "Past work has typically introduced specialized model architectures to address this.",
      "tokens": [
        "P",
        "a",
        "s",
        "t",
        " ",
        "w",
        "o",
        "r",
        "k",
        " ",
        "h",
        "a",
        "s",
        " ",
        "t",
        "y",
        "p",
        "i",
        "c",
        "a",
        "l",
        "l",
        "y",
        " ",
        "i",
        "n",
        "t",
        "r",
        "o",
        "d",
        "u",
        "c",
        "e",
        "d",
        " ",
        "s",
        "p",
        "e",
        "c",
        "i",
        "a",
        "l",
        "i",
        "z",
        "e",
        "d",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        " ",
        "a",
        "r",
        "c",
        "h",
        "i",
        "t",
        "e",
        "c",
        "t",
        "u",
        "r",
        "e",
        "s",
        " ",
        "t",
        "o",
        " ",
        "a",
        "d",
        "d",
        "r",
        "e",
        "s",
        "s",
        " ",
        "t",
        "h",
        "i",
        "s",
        "."
      ],
      "token_count": 83,
      "char_count": 83
    },
    {
      "text": "Third, tokenization adds substantial complexity to\nthe training and deployment pipeline.",
      "tokens": [
        "T",
        "h",
        "i",
        "r",
        "d",
        ",",
        " ",
        "t",
        "o",
        "k",
        "e",
        "n",
        "i",
        "z",
        "a",
        "t",
        "i",
        "o",
        "n",
        " ",
        "a",
        "d",
        "d",
        "s",
        " ",
        "s",
        "u",
        "b",
        "s",
        "t",
        "a",
        "n",
        "t",
        "i",
        "a",
        "l",
        " ",
        "c",
        "o",
        "m",
        "p",
        "l",
        "e",
        "x",
        "i",
        "t",
        "y",
        " ",
        "t",
        "o",
        "\n",
        "t",
        "h",
        "e",
        " ",
        "t",
        "r",
        "a",
        "i",
        "n",
        "i",
        "n",
        "g",
        " ",
        "a",
        "n",
        "d",
        " ",
        "d",
        "e",
        "p",
        "l",
        "o",
        "y",
        "m",
        "e",
        "n",
        "t",
        " ",
        "p",
        "i",
        "p",
        "e",
        "l",
        "i",
        "n",
        "e",
        "."
      ],
      "token_count": 88,
      "char_count": 88
    },
    {
      "text": "For example, ByT5-Small achieves an average GLUE score of 82.6, compared to 82.2 for\nT5-Small.",
      "tokens": [
        "F",
        "o",
        "r",
        " ",
        "e",
        "x",
        "a",
        "m",
        "p",
        "l",
        "e",
        ",",
        " ",
        "B",
        "y",
        "T",
        "5",
        "-",
        "S",
        "m",
        "a",
        "l",
        "l",
        " ",
        "a",
        "c",
        "h",
        "i",
        "e",
        "v",
        "e",
        "s",
        " ",
        "a",
        "n",
        " ",
        "a",
        "v",
        "e",
        "r",
        "a",
        "g",
        "e",
        " ",
        "G",
        "L",
        "U",
        "E",
        " ",
        "s",
        "c",
        "o",
        "r",
        "e",
        " ",
        "o",
        "f",
        " ",
        "8",
        "2",
        ".",
        "6",
        ",",
        " ",
        "c",
        "o",
        "m",
        "p",
        "a",
        "r",
        "e",
        "d",
        " ",
        "t",
        "o",
        " ",
        "8",
        "2",
        ".",
        "2",
        " ",
        "f",
        "o",
        "r",
        "\n",
        "T",
        "5",
        "-",
        "S",
        "m",
        "a",
        "l",
        "l",
        "."
      ],
      "token_count": 94,
      "char_count": 94
    },
    {
      "text": "The model is\ntrained to reconstruct spans of consecutive bytes that have been replaced with sentinel tokens.",
      "tokens": [
        "T",
        "h",
        "e",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        " ",
        "i",
        "s",
        "\n",
        "t",
        "r",
        "a",
        "i",
        "n",
        "e",
        "d",
        " ",
        "t",
        "o",
        " ",
        "r",
        "e",
        "c",
        "o",
        "n",
        "s",
        "t",
        "r",
        "u",
        "c",
        "t",
        " ",
        "s",
        "p",
        "a",
        "n",
        "s",
        " ",
        "o",
        "f",
        " ",
        "c",
        "o",
        "n",
        "s",
        "e",
        "c",
        "u",
        "t",
        "i",
        "v",
        "e",
        " ",
        "b",
        "y",
        "t",
        "e",
        "s",
        " ",
        "t",
        "h",
        "a",
        "t",
        " ",
        "h",
        "a",
        "v",
        "e",
        " ",
        "b",
        "e",
        "e",
        "n",
        " ",
        "r",
        "e",
        "p",
        "l",
        "a",
        "c",
        "e",
        "d",
        " ",
        "w",
        "i",
        "t",
        "h",
        " ",
        "s",
        "e",
        "n",
        "t",
        "i",
        "n",
        "e",
        "l",
        " ",
        "t",
        "o",
        "k",
        "e",
        "n",
        "s",
        "."
      ],
      "token_count": 108,
      "char_count": 108
    },
    {
      "text": "Byte sequences are\nsignificantly longer than token sequences (typically 3-8x longer), which increases computational cost.",
      "tokens": [
        "B",
        "y",
        "t",
        "e",
        " ",
        "s",
        "e",
        "q",
        "u",
        "e",
        "n",
        "c",
        "e",
        "s",
        " ",
        "a",
        "r",
        "e",
        "\n",
        "s",
        "i",
        "g",
        "n",
        "i",
        "f",
        "i",
        "c",
        "a",
        "n",
        "t",
        "l",
        "y",
        " ",
        "l",
        "o",
        "n",
        "g",
        "e",
        "r",
        " ",
        "t",
        "h",
        "a",
        "n",
        " ",
        "t",
        "o",
        "k",
        "e",
        "n",
        " ",
        "s",
        "e",
        "q",
        "u",
        "e",
        "n",
        "c",
        "e",
        "s",
        " ",
        "(",
        "t",
        "y",
        "p",
        "i",
        "c",
        "a",
        "l",
        "l",
        "y",
        " ",
        "3",
        "-",
        "8",
        "x",
        " ",
        "l",
        "o",
        "n",
        "g",
        "e",
        "r",
        ")",
        ",",
        " ",
        "w",
        "h",
        "i",
        "c",
        "h",
        " ",
        "i",
        "n",
        "c",
        "r",
        "e",
        "a",
        "s",
        "e",
        "s",
        " ",
        "c",
        "o",
        "m",
        "p",
        "u",
        "t",
        "a",
        "t",
        "i",
        "o",
        "n",
        "a",
        "l",
        " ",
        "c",
        "o",
        "s",
        "t",
        "."
      ],
      "token_count": 121,
      "char_count": 121
    },
    {
      "text": "When we\nrandomly drop 10% of characters, T5-Base's GLUE score drops by 8.2 points, while ByT5-Base's score\nonly drops by 3.1 points.",
      "tokens": [
        "W",
        "h",
        "e",
        "n",
        " ",
        "w",
        "e",
        "\n",
        "r",
        "a",
        "n",
        "d",
        "o",
        "m",
        "l",
        "y",
        " ",
        "d",
        "r",
        "o",
        "p",
        " ",
        "1",
        "0",
        "%",
        " ",
        "o",
        "f",
        " ",
        "c",
        "h",
        "a",
        "r",
        "a",
        "c",
        "t",
        "e",
        "r",
        "s",
        ",",
        " ",
        "T",
        "5",
        "-",
        "B",
        "a",
        "s",
        "e",
        "'",
        "s",
        " ",
        "G",
        "L",
        "U",
        "E",
        " ",
        "s",
        "c",
        "o",
        "r",
        "e",
        " ",
        "d",
        "r",
        "o",
        "p",
        "s",
        " ",
        "b",
        "y",
        " ",
        "8",
        ".",
        "2",
        " ",
        "p",
        "o",
        "i",
        "n",
        "t",
        "s",
        ",",
        " ",
        "w",
        "h",
        "i",
        "l",
        "e",
        " ",
        "B",
        "y",
        "T",
        "5",
        "-",
        "B",
        "a",
        "s",
        "e",
        "'",
        "s",
        " ",
        "s",
        "c",
        "o",
        "r",
        "e",
        "\n",
        "o",
        "n",
        "l",
        "y",
        " ",
        "d",
        "r",
        "o",
        "p",
        "s",
        " ",
        "b",
        "y",
        " ",
        "3",
        ".",
        "1",
        " ",
        "p",
        "o",
        "i",
        "n",
        "t",
        "s",
        "."
      ],
      "token_count": 132,
      "char_count": 132
    },
    {
      "text": "We showed that a\nstandard Transformer architecture can effectively process byte sequences, achieving competitive\nperformance with token-level models on diverse NLP tasks.",
      "tokens": [
        "W",
        "e",
        " ",
        "s",
        "h",
        "o",
        "w",
        "e",
        "d",
        " ",
        "t",
        "h",
        "a",
        "t",
        " ",
        "a",
        "\n",
        "s",
        "t",
        "a",
        "n",
        "d",
        "a",
        "r",
        "d",
        " ",
        "T",
        "r",
        "a",
        "n",
        "s",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        " ",
        "a",
        "r",
        "c",
        "h",
        "i",
        "t",
        "e",
        "c",
        "t",
        "u",
        "r",
        "e",
        " ",
        "c",
        "a",
        "n",
        " ",
        "e",
        "f",
        "f",
        "e",
        "c",
        "t",
        "i",
        "v",
        "e",
        "l",
        "y",
        " ",
        "p",
        "r",
        "o",
        "c",
        "e",
        "s",
        "s",
        " ",
        "b",
        "y",
        "t",
        "e",
        " ",
        "s",
        "e",
        "q",
        "u",
        "e",
        "n",
        "c",
        "e",
        "s",
        ",",
        " ",
        "a",
        "c",
        "h",
        "i",
        "e",
        "v",
        "i",
        "n",
        "g",
        " ",
        "c",
        "o",
        "m",
        "p",
        "e",
        "t",
        "i",
        "t",
        "i",
        "v",
        "e",
        "\n",
        "p",
        "e",
        "r",
        "f",
        "o",
        "r",
        "m",
        "a",
        "n",
        "c",
        "e",
        " ",
        "w",
        "i",
        "t",
        "h",
        " ",
        "t",
        "o",
        "k",
        "e",
        "n",
        "-",
        "l",
        "e",
        "v",
        "e",
        "l",
        " ",
        "m",
        "o",
        "d",
        "e",
        "l",
        "s",
        " ",
        "o",
        "n",
        " ",
        "d",
        "i",
        "v",
        "e",
        "r",
        "s",
        "e",
        " ",
        "N",
        "L",
        "P",
        " ",
        "t",
        "a",
        "s",
        "k",
        "s",
        "."
      ],
      "token_count": 170,
      "char_count": 170
    }
  ],
  "2105.13626_whitespace": [
    {
      "text": "We call the resulting models ByT5.",
      "tokens": [
        "We",
        "call",
        "the",
        "resulting",
        "models",
        "ByT5."
      ],
      "token_count": 6,
      "char_count": 34
    },
    {
      "text": "3 Approach\nWe base our models on T5, an encoder-decoder Transformer.",
      "tokens": [
        "3",
        "Approach",
        "We",
        "base",
        "our",
        "models",
        "on",
        "T5,",
        "an",
        "encoder-decoder",
        "Transformer."
      ],
      "token_count": 11,
      "char_count": 68
    },
    {
      "text": "We demonstrated that byte-level models\n\nare significantly more robust to noise.",
      "tokens": [
        "We",
        "demonstrated",
        "that",
        "byte-level",
        "models",
        "are",
        "significantly",
        "more",
        "robust",
        "to",
        "noise."
      ],
      "token_count": 11,
      "char_count": 79
    },
    {
      "text": "Past work has typically introduced specialized model architectures to address this.",
      "tokens": [
        "Past",
        "work",
        "has",
        "typically",
        "introduced",
        "specialized",
        "model",
        "architectures",
        "to",
        "address",
        "this."
      ],
      "token_count": 11,
      "char_count": 83
    },
    {
      "text": "Third, tokenization adds substantial complexity to\nthe training and deployment pipeline.",
      "tokens": [
        "Third,",
        "tokenization",
        "adds",
        "substantial",
        "complexity",
        "to",
        "the",
        "training",
        "and",
        "deployment",
        "pipeline."
      ],
      "token_count": 11,
      "char_count": 88
    },
    {
      "text": "For example, ByT5-Small achieves an average GLUE score of 82.6, compared to 82.2 for\nT5-Small.",
      "tokens": [
        "For",
        "example,",
        "ByT5-Small",
        "achieves",
        "an",
        "average",
        "GLUE",
        "score",
        "of",
        "82.6,",
        "compared",
        "to",
        "82.2",
        "for",
        "T5-Small."
      ],
      "token_count": 15,
      "char_count": 94
    },
    {
      "text": "The model is\ntrained to reconstruct spans of consecutive bytes that have been replaced with sentinel tokens.",
      "tokens": [
        "The",
        "model",
        "is",
        "trained",
        "to",
        "reconstruct",
        "spans",
        "of",
        "consecutive",
        "bytes",
        "that",
        "have",
        "been",
        "replaced",
        "with",
        "sentinel",
        "tokens."
      ],
      "token_count": 17,
      "char_count": 108
    },
    {
      "text": "Byte sequences are\nsignificantly longer than token sequences (typically 3-8x longer), which increases computational cost.",
      "tokens": [
        "Byte",
        "sequences",
        "are",
        "significantly",
        "longer",
        "than",
        "token",
        "sequences",
        "(typically",
        "3-8x",
        "longer),",
        "which",
        "increases",
        "computational",
        "cost."
      ],
      "token_count": 15,
      "char_count": 121
    },
    {
      "text": "When we\nrandomly drop 10% of characters, T5-Base's GLUE score drops by 8.2 points, while ByT5-Base's score\nonly drops by 3.1 points.",
      "tokens": [
        "When",
        "we",
        "randomly",
        "drop",
        "10%",
        "of",
        "characters,",
        "T5-Base's",
        "GLUE",
        "score",
        "drops",
        "by",
        "8.2",
        "points,",
        "while",
        "ByT5-Base's",
        "score",
        "only",
        "drops",
        "by",
        "3.1",
        "points."
      ],
      "token_count": 22,
      "char_count": 132
    },
    {
      "text": "We showed that a\nstandard Transformer architecture can effectively process byte sequences, achieving competitive\nperformance with token-level models on diverse NLP tasks.",
      "tokens": [
        "We",
        "showed",
        "that",
        "a",
        "standard",
        "Transformer",
        "architecture",
        "can",
        "effectively",
        "process",
        "byte",
        "sequences,",
        "achieving",
        "competitive",
        "performance",
        "with",
        "token-level",
        "models",
        "on",
        "diverse",
        "NLP",
        "tasks."
      ],
      "token_count": 22,
      "char_count": 170
    }
  ]
}